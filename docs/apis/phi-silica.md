---
title: Phi Silica in the Windows App SDK
description: Learn about the new Phi Silica APIs that will ship with the Windows App SDK and can be used to access local language models for local processing and generation of chat, math solving, code generation, reasoning over text, and more.
ms.topic: article
ms.date: 06/21/2024
ms.author: kbridge
author: karl-bridge-microsoft
dev_langs:
- csharp
- cpp
---

# Phi Silica in the Windows App SDK

Use the Windows App SDK to connect your Windows app to on-device language models, including Phi Silica, our most powerful NPU-tuned local language model yet.

The [Windows App SDK](/windows/apps/windows-app-sdk/) will ship with several Artificial Intelligence (AI) APIs to access these models and provide local processing and generation of chat, math solving, code generation, reasoning over text, and more.

> [!IMPORTANT]
> The Windows App SDK [experimental channel](/windows/apps/windows-app-sdk/experimental-channel) includes APIs and features in early stages of development. All APIs in the experimental channel are subject to extensive revisions and breaking changes and may be removed from subsequent releases at any time. They are not supported for use in production environments, and apps that use experimental features cannot be published to the Microsoft Store.

## Prerequisites

- [CoPilot+ PCs](/windows/ai/npu-devices/)

## Responsible AI

Phi Silica provides developers with a powerful, trustworthy model for building apps with safe, secure AI experiences. The following steps have been taken to ensure Phi Silica is trustworthy, secure, and built responsibly.

- Thorough testing and evaluation of the model quality to identify and mitigate potential risks.
- Creation of a Phi Silica model card that describes the strengths and limitations of the model and provides clarity about intended uses.
- Incremental roll out of Phi Silica experimental releases. Following the final Phi Silica experimental release, the roll out will expand to signed apps to ensure that malware scans have been applied to applications with local model capabilities.
- Provide customer controls through the Capability Access Manager in Settings so users can turn off the model on the device for the system, user, or app.

> [!NOTE]
> Content moderation is currently in development.

In addtion to the above, we recommend reviewing the best practices described in [Responsible Generative AI Development on Windows](/windows/ai/rai).

## What can I do with Phi Silica and the Windows App SDK?

With a local Phi Silica language model and the Windows App SDK you can generate text responses to user prompts.

### Get a single, complete response based on a string prompt

This example shows how to generate a response to a Q&A prompt where the full response is generated before the result is returned.

1. First, we ensure the language model is available by calling the [IsAvailable](phi-silica-api-ref.md#microsoftwindowsaigenerativelanguagemodelisavailable-method) method and waiting for the [MakeAvailableAsync](phi-silica-api-ref.md#microsoftwindowsaigenerativelanguagemodelmakeavailableasync-method) method to return successfully.
1. Once the language model is available, we create a [LanguageModel](phi-silica-api-ref.md#microsoftwindowsaigenerativelanguagemodel-class) object to reference it.
1. Finally, we submit a string prompt to the model using the [GenerateResponseAsync](phi-silica-api-ref.md#microsoftwindowsaigenerativelanguagemodelgenerateresponseasyncsystemstring-method) method, which returns the complete result.

```csharp
using Microsoft.Windows.AI.Generative; 
 
 
if (!LanguageModel.IsAvailable()) 
{ 
   var op = await LanguageModel.MakeAvailableAsync(); 
} 
 
using LanguageModel languageModel = await LanguageModel.CreateAsync(); 
 
string prompt = "Provide the molecular formula for glucose."; 
 
var result = await languageModel.GenerateResponseAsync(prompt); 
 
Console.WriteLine(result.Response); 
```

```cpp
using namespace winrt::Microsoft::Windows::AI::Generative;

if (!LanguageModel::IsAvailable()) 
{
    auto op = LanguageModel::MakeAvailableAsync().get();
}

auto languageModel = LanguageModel::CreateAsync().get();

std::string prompt = "Provide the molecular formula for glucose.";

auto result = languageModel.GenerateResponseAsync(prompt).get();

std::cout << result.Response << std::endl;
```

The response generated by this example is:

```output
The molecular formula for glucose is C6H12O6.
```

### Get a stream of partial results based on a string prompt

This example shows how to generate a response to a Q&A prompt where the response is returned as a stream of partial results.

1. First we create a [LanguageModel](phi-silica-api-ref.md#microsoftwindowsaigenerativelanguagemodel-class) object to reference the local language model (we already checked for the presence of the language model in the previous snippet).
1. Then we asynchronously retrieve the [LanguageModelResponse](phi-silica-api-ref.md#microsoftwindowsaigenerativelanguagemodelresponse-class) in a call to [GenerateResponseWithProgressAsync](phi-silica-api-ref.md#microsoftwindowsaigenerativelanguagemodelgenerateresponsewithprogressasyncsystemstring-method) and write it to the console as the response is generated.

```csharp
using Microsoft.Windows.AI.Generative; 

using LanguageModel languageModel = await LanguageModel.CreateAsync(); 
 
 string prompt = "Provide the molecular formula for glucose."; 
 
  AsyncOperationProgressHandler<LanguageModelResponse, string> 
 progressHandler = (asyncInfo, delta) => 
 { 
     Console.WriteLine($"Progress: {delta}"); 
     Console.WriteLine($"Response so far: {asyncInfo.GetResults().Response()}"); 
 }; 
 
var asyncOp = languageModel.GenerateResponseWithProgressAsync(prompt); 
 
 asyncOp.Progress = progressHandler; 
 
 var result = await asyncOp;  
 
 Console.WriteLine(result.Response);
```

```cpp
using namespace winrt::Microsoft::Windows::AI::Generative;

auto languageModel = LanguageModel::CreateAsync().get();

std::string prompt = "Provide the molecular formula for glucose.";

AsyncOperationProgressHandler<LanguageModelResponse, std::string> progressHandler = 
    [](const IAsyncOperationWithProgress<LanguageModelResponse, std::string>& asyncInfo, const std::string& delta) 
    { 
        std::cout << "Progress: " << delta << std::endl; 
        std::cout << "Response so far: " << asyncInfo.GetResults().Response() << std::endl; 
    };

auto asyncOp = languageModel.GenerateResponseWithProgressAsync(prompt);

asyncOp.Progress(progressHandler); 

auto result = asyncOp.get();

std::cout << result.Response() << std::endl;
```

## Additional resources

[Access files and folders with Windows App SDK and WinRT APIs](/windows/apps/develop/files/winrt-files)

## Related content

- [API ref for Phi Silica APIs in the Windows App SDK](phi-silica-api-ref.md)
- [Windows App SDK](/windows/apps/windows-app-sdk/)
- [Latest release notes for the Windows App SDK](/windows/apps/windows-app-sdk/release-channels)
